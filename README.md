# Neural Network Basics
This repo contains a basic implementation of a neural network for own learning purposes implemented completely in numpy. This means, that the implementation is not as efficient as other libraries of course, but should implement some important papers in an intuitive and easy to read way.

Basic usages can be found in the ['examples' folder](examples/).

## Done
- Fully Connected Layer
- Initializations, Activations, Loss-functions
- SGD
- Convolutional Layers (2D)
- Max-Pooling (2D)

## WIP
- AdaGrad, RMSProp, Adam, Lazy-Adam
- Average-Pooling
- Batch-Norm, Layer-Norm
- Dropout

## Future
- RNN / LSTM
- Attention mechanisms
- Unpooling